# Random Lines Web Service

A small web service that:

* **Uploads** plain text files
* Returns **one random line** (content negotiation: `text/plain`, `application/json`, `application/xml`)
* Returns **one random line backwards**
* Returns the **100 longest lines** across all uploads
* Returns the **20 longest lines** for a specific file

This README documents **assumptions**, **technical & non‑technical requirements**, **design decisions**, **API contracts**, and **how to run**. It’s intended to make the reviewer immediately productive when evaluating the solution.

---

## Project Structure

A high-level overview of the project's layout.

```
File-Upload-Code-Challenge/
│
├── app.py                     # Flask entry point – registers all blueprints and runs the server
├── requirements.txt          # Python package dependencies
├── README.md                 # Project documentation (this file)
│
├── api/                      # Main application package
│   ├── __init__.py
│   │
│   ├── models/               # Business logic layer (no Flask request/response objects)
│   │   ├── __init__.py
│   │   ├── file_model.py     # Handles file upload, storage, and indexing
│   │   ├── line_model.py     # Logic for selecting random lines
│   │   └── longest_model.py  # Logic for computing longest lines
│   │
│   ├── views/                # Flask API layer (request parsing & response formatting)
│   │   ├── __init__.py
│   │   ├── upload_views.py   # POST /files
│   │   ├── line_views.py     # GET /lines/random
│   │   └── longest_views.py  # GET /lines/longest
│   │
│   └── utils/                # Reusable utility modules
│       ├── __init__.py
│       ├── db.py             # SQLite metadata storage
│       ├── indexing.py       # Streaming file index builder
│       ├── reader.py         # Helpers to read lines by byte offsets
│       ├── response.py       # Content negotiation & XML generator
│       ├── storage.py        # Abstraction for local & Cloudflare R2 storage
│       └── textutils.py      # Text helpers (e.g., most frequent letter)
│
├── scripts/                  # Helper scripts (not part of the API runtime)
│   └── make_big_files.py     # Script to generate test files with long lines
│
├── uploads/                  # Uploaded files (runtime, in local mode)
│
├── indexes/                  # Binary index files (runtime)
│
├── data.db                   # SQLite database file (autogenerated at runtime)
│
└── tests/                    # (Optional) Test folder for pytest
    └── test_app.py
```

## Stack Choice (and a quick correction)

> The team I’m applying to uses **Django**. For this task, an async‑friendly microframework like **FastAPI** or **Node.js (Fastify/Nest)** would be optimal for streaming and content negotiation. However, I chose **Flask** because it’s lightweight and conceptually close to Django’s request/response mental model, making the codebase familiar and easy to review for a Django team.

**Why Flask here**

* Minimal boilerplate, readable single‑file core

**If this were production**

* I would consider **FastAPI** for first‑class async I/O and automatic validation docs, or **Django/DRF** if we needed admin/auth/ORM features out of the box.

---

## Commit Message Conventions

Each commit message **must start** with one of the following symbols to indicate the nature of the change:

* `+` **Added**: new files, features, endpoints, tests, docs, or configuration.
* `-` **Removed**: deletions of files, features, flags, or dead code.
* `*` **Modified**: changes to existing code, refactors, fixes, or behavior tweaks.

---

## Assumptions

1. **File type**: input files are **UTF‑8 text** (binary files are out of scope). Non‑UTF‑8 bytes are replaced with the Unicode replacement character when decoding.
2. **Line definition**: a line is **bytes ending with `\n`**. The final line **may not** end with `\n` and is still considered a line.
3. **Large files**: files may contain **millions of lines**. We must support random access without loading the whole file into RAM.
4. **Indexing strategy**: we use **chunk‑based indexing** (offset every *K* lines, default `K=1000`) to avoid writing one DB row per line.
5. **Storage**: by default the submission can run locally with filesystem/SQLite, and optionally use **Cloudflare R2** (S3‑compatible) for object storage.
6. **Content negotiation**: the service supports `text/plain`, `application/json`, `application/xml`. If the `Accept` header includes `application/*`, metadata is returned.
7. **Default content type**: if no `Accept` header is provided, the service defaults to `application/json`.
8. **Security**: unauthenticated demo service.
9. **Limits**: per‑request `limit` is capped (1…1000) to prevent misuse. Upload size can be limited by web server or reverse proxy config.

---

## Technical Requirements

1. **Upload**: `POST /files` (multipart) — stream to storage, compute **chunk index** while writing, store metadata in SQLite.
2. **Random line**: `GET /lines/random` — content negotiation; if `application/*` -> include metadata:

   * `file_name`
   * `line_number`
   * `most_frequent_letter` (a–z, case‑insensitive; see Edge Cases for special behavior)
3. **Random line (backwards)**: `GET /lines/random/backwards` — same selection, reversed string.
4. **Longest 100 lines (all files)**: `GET /lines/longest?limit=100` — min‑heap streaming scan across files.
5. **20 longest lines (one file)**: `GET /lines/longest?file_name=<name>&limit=20`.
6. **Content types**: support `text/plain`, `application/json`, `application/xml`.
7. **Scalability**: work with multi‑GB files without reading entire files into memory.

---

## Design Overview

### Storage

* **Option A (local)**: files in `./uploads/`, index in `./indexes/`, metadata in `./data.db` (SQLite).
* **Option B (Cloudflare R2)**: objects under `uploads/<filename>`, index under `indexes/<filename>.idx`, metadata still in `SQLite` (only file‑level, not per‑line).

### Chunk‑Based Indexing

* On upload, stream bytes and track **byte offsets** of line starts.
* Record the start offset every **K lines** (`K=1000` by default). The offset of line 0 is always 0.
* To fetch line *L*:

  * Compute `base = floor(L/K)*K` and `advance = L - base`.
  * Load the small index; `start_offset = index[base/K]`.
  * Range‑read from `start_offset`, **skip `advance` newlines**, then capture that line only.
  * **Example (numbers):** with `K=1000` and target line `L=3456`:

    * `base = floor(3456/1000)*1000 = 3000`
    * `advance = 3456 - 3000 = 456`
    * `start_offset = index[3000/1000] = index[3]` (byte position where line 3000 starts)
    * Range‑read from `start_offset`, skip **456** newline characters; the bytes up to the next `
      ` are the contents of line **3456**.
* **Benefits**: tiny index (~8 bytes × N/K), fast random access, no huge DB tables.

### Longest Lines

* Stream lines from storage, keep a **min‑heap** of size `limit` with entries `(length, file, line_no, text)`.
* Complexity: O(total_lines × log limit). For the default `limit=100`, log factor is tiny.

### Content Negotiation

* Inspect `Accept` header, choose response type.
* If client requests `application/*`, include metadata fields in JSON/XML bodies.
* For `text/plain`, return raw line(s) only (no metadata).

---

## API Reference

### `POST /files`

Upload a text file.

**Request**: `multipart/form-data`, field `file=@/path/to/file.txt`

**Response 200 (JSON)**

```json
{
  "filename": "lorem.txt",
  "size_bytes": 123456,
  "num_lines": 4200,
  "lines_per_chunk": 1000,
  "object_key": "uploads/lorem.txt",
  "idx_key": "indexes/lorem.txt.idx"
}
```

---

### `GET /lines/random`

Return one random line across all files (or specify `?file_name=`).

**Query Params**

* `file_name` (optional): restrict to specific file.

**Accept: `text/plain`** → returns just the line text.

**Accept: `application/json` or `application/xml`** → returns metadata:

```json
{
  "file_name": "lorem.txt",
  "line_number": 1337,
  "line": "Veni, vidi, vici.",
  "most_frequent_letter": "i"
}
```

---

### `GET /lines/random/backwards`

Same as above, but the selected line is reversed.

**Accept: `text/plain`** → returns reversed text only.

**Accept: `application/json`/`xml`** →

```json
{
  "file_name": "lorem.txt",
  "line_number": 1337,
  "line_reversed": ".iciv ,idiv ,ineV",
  "most_frequent_letter": "i"
}
```

---

### `GET /lines/longest`

Return the longest lines.

**Query Params**

* `limit` (optional, default `100`, range `1..1000`)
* `file_name` (optional): if provided, returns longest `limit` lines of that file; otherwise across all files.

**Accept: `text/plain`** → newline‑joined lines only.

**Accept: `application/json`/`xml`** → array of objects:

```json
[
  { "length": 512, "file_name": "a.txt", "line_number": 9012, "line": "..." },
  { "length": 511, "file_name": "b.txt", "line_number": 77,   "line": "..." }
]
```

---

## Testing

This section covers both automated and manual testing procedures.

### Automated Testing

The test suite uses `pytest`. To run the tests, navigate to the `tests/` directory and execute:

```bash
pytest -q
```

### Manual Testing

Manual testing can be performed using `curl` to interact with the running service.

#### 1. Prepare Test Files

You can generate large test files using the `make_large_files.py` script. To create a file named `large.txt` with 1 million lines:

```bash
python make_large_files.py --filename large.txt --lines 1000000
```

#### 2. API Endpoint Tests

The following commands cover the main API endpoints. Replace `your.txt` with the name of a file you have uploaded.

**A. Upload a File**
```bash
curl -F "file=@/path/to/your.txt" http://127.0.0.1:8000/files
```

**B. Get a Random Line**
*   **Plain Text**: `curl -H "Accept: text/plain" http://127.0.0.1:8000/lines/random`
*   **JSON**: `curl -H "Accept: application/json" http://127.0.0.1:8000/lines/random`
*   **XML**: `curl -H "Accept: application/xml" http://127.0.0.1:8000/lines/random`

**C. Get a Random Line Backwards**
*   **Plain Text**: `curl -H "Accept: text/plain" http://127.0.0.1:8000/lines/random/backwards`
*   **JSON**: `curl -H "Accept: application/json" http://127.0.0.1:8000/lines/random/backwards`
*   **XML**: `curl -H "Accept: application/xml" http://127.0.0.1:8000/lines/random/backwards`

**D. Get Longest Lines (Across All Files)**
*   **Default (100 lines, JSON)**: `curl http://127.0.0.1:8000/lines/longest`
*   **With a custom limit (e.g., 5 lines)**:
    *   **JSON**: `curl "http://127.0.0.1:8000/lines/longest?limit=5"`
    *   **Plain Text**: `curl -H "Accept: text/plain" "http://127.0.0.1:8000/lines/longest?limit=5"`
    *   **XML**: `curl -H "Accept: application/xml" "http://127.0.0.1:8000/lines/longest?limit=5"`

**E. Get Longest Lines (For a Specific File)**
*   **Default (20 lines, JSON)**: `curl "http://127.0.0.1:8000/lines/longest?file_name=your.txt"`
*   **With a custom limit (e.g., 10 lines)**:
    *   **JSON**: `curl "http://127.0.0.1:8000/lines/longest?file_name=your.txt&limit=10"`
    *   **Plain Text**: `curl -H "Accept: text/plain" "http://127.0.0.1:8000/lines/longest?file_name=your.txt&limit=10"`
    *   **XML**: `curl -H "Accept: application/xml" "http://127.0.0.1:8000/lines/longest?file_name=your.txt&limit=10"`

**F. Additional Test Cases**
*   **Get a random line from a specific file (JSON)**: `curl -H "Accept: application/json" "http://127.0.0.1:8000/lines/random?file_name=your.txt"`
*   **Test `limit` clamping (requesting 2000, should be clamped to 1000)**: `curl "http://127.0.0.1:8000/lines/longest?limit=2000"`

---

## Edge Cases & Behavior

* Empty file → `num_lines = 0`; random‑line endpoints return 404
* File with no trailing `\n` → last line still counted
* Mixed encodings → undecodable bytes replaced during UTF‑8 decode
* Only one file uploaded → random across that file’s lines
* `limit` outside 1..1000 → clamped to bounds
* **`GET /lines/longest` behavior**:
    * If `file_name` is specified without a `limit`, it returns the 20 longest lines for that file.
    * If `limit` is specified without a `file_name`, it returns that number of longest lines from across all files.
    * If neither is specified, it returns the 100 longest lines from across all files.
* **`most_frequent_letter` logic**:
    * If a line has no letters (e.g., "12345"), the value is `"N/A"`.
    * If all letters in a line have the same frequency (e.g., "aabb" or "abc"), the value is `"Tie"`.

    
---

## Future Enhancements

While the current implementation meets the core requirements, several enhancements could be made for a production-grade service:

*   **Containerization**: The application could be containerized using Docker for consistent, isolated deployments and easier orchestration (e.g., with Kubernetes).
*   **Simple UI**: A lightweight front-end (e.g., using a simple HTML/CSS/JS page or a framework like Vue/React) could be added to provide a user-friendly interface for uploading files and viewing results.
*   **Cloud Storage Integration**: The existing design for S3/Cloudflare R2 compatibility could be fully integrated and made the default storage backend, improving scalability and decoupling storage from the application server.
*   **Asynchronous Processing**: For extremely large files, the upload and indexing process could be offloaded to a background task queue (e.g., Celery with Redis/RabbitMQ) to provide a more responsive API.
*   **Enhanced Security**: Implementing API key authentication and authorization would protect the service from unauthorized access and misuse.
*   **Observability**: Integrating structured logging, metrics (e.g., Prometheus), and tracing (e.g., OpenTelemetry) would provide deep insights into application performance and behavior.